# ç®€åŒ–ç‰ˆ GKD è®­ç»ƒæŒ‡å—

è¿™æ˜¯ä¸€ä¸ªåŸºäº PPO-style çš„ç®€åŒ– GKD (Generalized Knowledge Distillation) è®­ç»ƒå®ç°ï¼Œç›¸æ¯”åŸæœ‰çš„ `recipe/gkd` å®ç°æ›´åŠ ç®€å•æ˜“ç”¨ã€‚

## ğŸ¯ ä¸»è¦ç‰¹æ€§

- **ç®€åŒ–æ¶æ„**ï¼šä½¿ç”¨ FSDP åç«¯è€Œé Megatronï¼Œé…ç½®æ›´ç®€å•
- **åŒæ­¥è°ƒç”¨**ï¼šTeacher çŸ¥è¯†è·å–é‡‡ç”¨åŒæ­¥æ–¹å¼ï¼Œå®ç°ç®€å•æ¸…æ™°
- **çµæ´»æŸå¤±ç»„åˆ**ï¼šæ”¯æŒçº¯è’¸é¦æˆ– RL+è’¸é¦æ··åˆè®­ç»ƒ
- **å¤ç”¨ GRPO å·¥å…·**ï¼šç›´æ¥ä½¿ç”¨ GRPO çš„ agent äº¤äº’å’Œ rollout é…ç½®

## ğŸ“ æ–‡ä»¶ç»“æ„

```
verl/
â”œâ”€â”€ trainer/
â”‚   â”œâ”€â”€ gkd/                          # GKD è®­ç»ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ray_trainer.py            # RayGKDTrainer (ç»§æ‰¿ RayPPOTrainer)
â”‚   â”‚   â””â”€â”€ distill_loss.py           # FSDP å…¼å®¹çš„è’¸é¦æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ main_gkd.py                   # GKD è®­ç»ƒå…¥å£ç‚¹
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ gkd_trainer.yaml          # GKD é…ç½®æ–‡ä»¶
â”œâ”€â”€ workers/
â”‚   â””â”€â”€ actor/
â”‚       â””â”€â”€ dp_actor.py               # æ‰©å±•æ”¯æŒè’¸é¦æŸå¤±
â””â”€â”€ baseline/
    â””â”€â”€ GKD/
        â”œâ”€â”€ run_gkd_simple.sh         # ç®€åŒ–å¯åŠ¨è„šæœ¬
        â”œâ”€â”€ teacher/                   # Teacher server (å¤ç”¨åŸæœ‰)
        â””â”€â”€ README_SIMPLE.md          # æœ¬æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨ Teacher Server

é¦–å…ˆå¯åŠ¨ teacher model serverï¼ˆä½¿ç”¨ GPU 0-1ï¼‰ï¼š

```bash
cd baseline/GKD/teacher
export CUDA_VISIBLE_DEVICES=0,1
bash start_server.sh
```

éªŒè¯ server å·²å¯åŠ¨ï¼š

```bash
telnet localhost 15555
```

### 2. è¿è¡Œ GKD è®­ç»ƒ

åœ¨å¦ä¸€ä¸ªç»ˆç«¯ä¸­ï¼Œå¯åŠ¨ student è®­ç»ƒï¼ˆä½¿ç”¨ GPU 2-7ï¼‰ï¼š

```bash
cd /root/yuxiang/verl
bash baseline/GKD/run_gkd_simple.sh
```

## âš™ï¸ é…ç½®è¯´æ˜

### GKD æ ¸å¿ƒé…ç½®

åœ¨ `verl/trainer/config/gkd_trainer.yaml` ä¸­ï¼š

```yaml
gkd:
  # Teacher é…ç½®
  enable_teacher: true
  teacher_ip: "127.0.0.1"
  teacher_port: 15555
  teacher_topk: 256
  
  # è’¸é¦æŸå¤±é…ç½®
  distill_loss_coef: 1.0
  distill_loss_type: "forward_kl"  # forward_kl, reverse_kl, jsd
  distill_temperature: 1.0
  
  # RL æŸå¤±é…ç½®ï¼ˆå¯é€‰ï¼‰
  use_rl_loss: false
  rl_loss_coef: 0.0
```

### æŸå¤±ç±»å‹è¯´æ˜

- **forward_kl**: `KL(teacher||student)` - é¼“åŠ± student è¦†ç›– teacher çš„æ‰€æœ‰æ¨¡å¼
- **reverse_kl**: `KL(student||teacher)` - é¼“åŠ± student èšç„¦äº teacher çš„ä¸»è¦æ¨¡å¼
- **jsd**: Jensen-Shannon Divergence - å¯¹ç§°çš„å¹³è¡¡æ–¹æ³•

### è®­ç»ƒæ¨¡å¼

#### æ¨¡å¼ 1: çº¯è’¸é¦ï¼ˆæ¨èï¼‰

```bash
gkd.use_rl_loss=false \
gkd.distill_loss_coef=1.0
```

#### æ¨¡å¼ 2: RL + è’¸é¦æ··åˆ

```bash
gkd.use_rl_loss=true \
gkd.rl_loss_coef=0.1 \
gkd.distill_loss_coef=1.0
```

## ğŸ” ä¸åŸæœ‰å®ç°çš„å¯¹æ¯”

| ç‰¹æ€§ | åŸæœ‰ `recipe/gkd` | ç®€åŒ–ç‰ˆ `baseline/GKD` |
|------|-------------------|----------------------|
| åç«¯ | Megatron | FSDP |
| Teacher è°ƒç”¨ | å¼‚æ­¥ (one_step_off) | åŒæ­¥ |
| é…ç½®å¤æ‚åº¦ | é«˜ | ä½ |
| ä»£ç å¤æ‚åº¦ | é«˜ï¼ˆè‡ªå®šä¹‰ workerï¼‰ | ä½ï¼ˆå¤ç”¨ PPOï¼‰ |
| æ€§èƒ½ä¼˜åŒ– | é«˜ï¼ˆæµæ°´çº¿ä¼˜åŒ–ï¼‰ | ä¸­ï¼ˆåŒæ­¥è°ƒç”¨ï¼‰ |
| æ˜“ç”¨æ€§ | ä½ | é«˜ |
| é€‚ç”¨åœºæ™¯ | ç”Ÿäº§ç¯å¢ƒå¤§è§„æ¨¡è®­ç»ƒ | ç ”ç©¶ã€å®éªŒã€å°è§„æ¨¡è®­ç»ƒ |

## ğŸ“Š ç›‘æ§æŒ‡æ ‡

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè®°å½•ä»¥ä¸‹æŒ‡æ ‡ï¼š

- `actor/distill_loss`: è’¸é¦æŸå¤±å€¼
- `actor/distill_coef`: è’¸é¦æŸå¤±ç³»æ•°
- `actor/pg_loss`: ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼ˆå¦‚æœå¯ç”¨ RLï¼‰
- `actor/entropy`: ç­–ç•¥ç†µ
- `training/global_step`: å…¨å±€è®­ç»ƒæ­¥æ•°

## ğŸ› ï¸ è‡ªå®šä¹‰ä¿®æ”¹

### 1. ä¿®æ”¹è’¸é¦æŸå¤±

ç¼–è¾‘ `verl/trainer/gkd/distill_loss.py` ä¸­çš„ `compute_fsdp_kl_divergence` å‡½æ•°ã€‚

### 2. ä¿®æ”¹è®­ç»ƒæµç¨‹

ç¼–è¾‘ `verl/trainer/gkd/ray_trainer.py` ä¸­çš„ `RayGKDTrainer.fit()` æ–¹æ³•ã€‚

### 3. æ·»åŠ è‡ªå®šä¹‰ reward

åœ¨å¯åŠ¨è„šæœ¬ä¸­è®¾ç½®ï¼š

```bash
custom_reward_function.path=path/to/your/reward.py \
custom_reward_function.name=your_reward_function
```

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: Teacher server è¿æ¥å¤±è´¥

**ç—‡çŠ¶**: `Teacher request failed` é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
1. ç¡®è®¤ teacher server å·²å¯åŠ¨ï¼š`telnet localhost 15555`
2. æ£€æŸ¥ GPU åˆ†é…æ˜¯å¦å†²çª
3. æŸ¥çœ‹ teacher server æ—¥å¿—

### é—®é¢˜ 2: OOM é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å° `ppo_micro_batch_size_per_gpu`
2. å¯ç”¨ `param_offload` å’Œ `optimizer_offload`
3. å‡å° `ppo_max_token_len_per_gpu`

### é—®é¢˜ 3: è’¸é¦æŸå¤±ä¸º 0

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥ `has_teacher_knowledge` æ˜¯å¦ä¸º True
2. ç¡®è®¤ teacher_topk_logps åœ¨ batch ä¸­
3. æ£€æŸ¥ distill_loss_coef æ˜¯å¦ä¸º 0

## ğŸ“ å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬å®ç°ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{verl_gkd_simple,
  title = {Simplified GKD Training for veRL},
  author = {veRL Team},
  year = {2025},
  url = {https://github.com/volcengine/verl}
}
```

## ğŸ“„ è®¸å¯è¯

Apache License 2.0

---

**æ³¨æ„**: æœ¬å®ç°ä¼˜å…ˆè€ƒè™‘ç®€æ´æ€§å’Œæ˜“ç”¨æ€§ã€‚å¦‚éœ€ç”Ÿäº§çº§æ€§èƒ½ä¼˜åŒ–ï¼Œè¯·ä½¿ç”¨åŸæœ‰çš„ `recipe/gkd` å®ç°ã€‚
