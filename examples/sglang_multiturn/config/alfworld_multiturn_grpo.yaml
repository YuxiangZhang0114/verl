# ALFWorld Multi-turn GRPO Training Configuration
# Simplified version - most parameters are set via command line

hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

# Data configuration
data:
  max_prompt_length: 2048
  max_response_length: 8192
  train_batch_size: 128
  return_raw_chat: True
  filter_overlong_prompts: True
  truncation: error

# Actor, Rollout, and Reference model configuration
actor_rollout_ref:
  hybrid_engine: True
  
  rollout:
    name: sglang
    
    agent:
      default_agent_loop: tool_agent
    
    multi_turn:
      enable: True
      max_assistant_turns: 20
      max_user_turns: 20
      max_parallel_calls: 1
      max_tool_response_length: 1024
      tool_response_truncate_side: right
      format: hermes
      interaction_config_path: null

# Algorithm configuration
algorithm:
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: True
  gamma: 1.0
  lam: 0.95
  use_kl_in_reward: False

# Trainer configuration
trainer:
  total_epochs: 10
  project_name: alfworld-grpo
  logger: ["console", "wandb"]
  nnodes: 1
  n_gpus_per_node: 8
  save_freq: 100
  test_freq: 50
  val_before_train: True
  critic_warmup: 0

# Reward model configuration (rule-based)
reward_model:
  enable: False

# Reward manager configuration
reward_manager:
  reward_fn_type: rule
  reward_fn_path: null
